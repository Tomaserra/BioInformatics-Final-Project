---
title: "**Alzheimer's Disease Prediction Using DNA Methylation Markers**"
author: "Marco Banfi, Simone Carrieri, Tomás Serra"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 10, fig.height = 6)
```

This project develops a predictive model for Alzheimer's Disease (AD) using DNA methylation data from post-mortem brain tissue. After controlling for demographic and technical confounders we use logistic regression to identify which methylation markers can distinguish AD patients from healthy controls.

**TO BE CHECKED Key Findings:**

-   Analyzed 485,577 CpG methylation sites from 190 samples
-   Identified significant methylation changes associated with AD progression
-   Built a logistic regression model with train/test validation
-   Model saved as `alzheimers_model.rds` for API deployment

------------------------------------------------------------------------

# **Introduction**

## **Alzheimer's Disease Background** [1]

Alzheimer's Disease (AD) is a non-monogenic progressive neurodegenerative disorder characterized by:

-   **Memory loss** and cognitive decline
-   **Pathological hallmarks**: Amyloid-beta plaques and neurofibrillary tangles
-   **Braak staging**: A classification system (0-6) measuring disease progression based on tau pathology distribution

## **DNA Methylation in Alzheimer's Disease**

**What is DNA methylation?**

DNA methylation is an epigenetic modification where methyl groups (CH₃) are added to cytosine bases, typically at CpG dinucleotides. This modification regulates gene expression without changing the DNA sequence.[2]

**Why methylation matters in AD:**

-   **Epigenetic dysregulation**: AD brains show altered methylation patterns
-   **Gene expression changes**: Methylation affects expression of genes involved in inflammation, synaptic function, and neuronal survival
-   **Potential biomarkers**: Specific methylation changes may serve as disease indicators or therapeutic targets

Recent studies have identified differential methylation in AD-related genes including: [3]

-   **APOE** (apolipoprotein E) - major genetic risk factor
-   **APP** (amyloid precursor protein) - involved in plaque formation
-   **MAPT** (microtubule-associated protein tau) - forms tangles
-   Inflammatory response genes

------------------------------------------------------------------------

## **Dataset Description: GSE66351**

### **Data Source** [4]

-   **Repository**: Gene Expression Omnibus (GEO)
-   **Accession**: GSE66351
-   **Platform**: Illumina HumanMethylation450 BeadChip
-   **Citation**: Lunnon et al. (2014) - "Methylomic profiling implicates cortical deregulation of ANK1 in Alzheimer's disease"

### **Sample Characteristics**

**Total samples**: 190 post-mortem brain tissue samples

**Diagnosis groups:**

-   **Alzheimer's Disease (AD)**: Cases with confirmed AD pathology
-   **Controls**: Age-matched individuals without AD pathology

**Clinical variables:**

1.  **Diagnosis**: AD vs Control (our outcome variable)
2.  **Age**: Patient age at death (continuous, years)
3.  **Sex**: Male or Female (categorical)
4.  **Braak stage**: AD progression severity (0-6, ordinal)
    -   0: No tau pathology
    -   I-II: Transentorhinal
    -   III-IV: Limbic
    -   V-VI: Neocortical (severe AD)
5.  **Cell type**: Neuronal, glial, or bulk tissue (categorical)
6.  **Brain region**: Cortical region sampled (categorical)

### **Methylation Data**

**CpG sites**: 481, 868 sites across the genome

**Measurement**: Beta values (0-1 scale)

-   **0** = No methylation (unmethylated)
-   **1** = Complete methylation (fully methylated)
-   **0.5** = 50% of cells methylated at that site

**Coverage**: Covers gene promoters, gene bodies, CpG islands, and intergenic regions

------------------------------------------------------------------------

# **Methods**

## **Data Acquisition and Preprocessing**

```{r install-packages, results='hide'}
# Install required packages
if (!require("BiocManager", quietly = TRUE))
  install.packages("BiocManager")

if (!require("GEOquery", quietly = TRUE))
  BiocManager::install("GEOquery")

if (!require("caret", quietly = TRUE))
  install.packages("caret")

# Load libraries
library(GEOquery)
library(dplyr)
library(ggplot2)
library(caret)

set.seed(123)  # Reproducibility
```

```{r download-data, message=FALSE, warning=FALSE}
options(timeout = 6000)

data_dir <- "data"
if (!dir.exists(data_dir)) dir.create(data_dir)

gse_file <- file.path(data_dir, "GSE66351.rds")

if (!file.exists(gse_file)) {
  message("Downloading GSE66351 from GEO (this may take several minutes)...")
  
  gse <- GEOquery::getGEO(
    "GSE66351",
    GSEMatrix = TRUE,
    AnnotGPL = FALSE,
    getGPL = FALSE
  )[[1]]
  
  saveRDS(gse, gse_file)
} else {
  message("Loading cached GSE66351 data...")
  gse <- readRDS(gse_file)
}

# Extract data
pheno_data <- pData(phenoData(gse))
methylation_data <- exprs(gse)
```

**Dataset dimensions:**

-   CpG sites: `r nrow(methylation_data)`
-   Samples: `r ncol(methylation_data)`

### **Clinical Data Preparation**

```{r clinical-data}
# Extract and clean clinical variables
clinical_data <- data.frame(
  sample_id = pheno_data$geo_accession,
  diagnosis = gsub("disease state: ", "", gsub(".*: ", "", pheno_data$characteristics_ch1.1)),
  age = as.numeric(gsub(".*: ", "", pheno_data$characteristics_ch1.4)),
  sex = gsub(".*: ", "", pheno_data$characteristics_ch1.5),
  cell_type = gsub(".*: ", "", pheno_data$characteristics_ch1),
  braak_stage = gsub(".*: ", "", pheno_data$characteristics_ch1.2),
  region = gsub(".*: ", "", pheno_data$characteristics_ch1.3)
)

# Create binary outcome (1 = AD, 0 = Control)
clinical_data$AD_status <- ifelse(clinical_data$diagnosis == "AD", 1, 0)

# Convert to factors
clinical_data$sex <- factor(clinical_data$sex)
clinical_data$cell_type <- factor(clinical_data$cell_type)
clinical_data$region <- factor(clinical_data$region)
```

```{r clinical-summary}
# Summary statistics
summary_stats <- list(
  n_total = nrow(clinical_data),
  n_ad = sum(clinical_data$AD_status),
  n_control = sum(clinical_data$AD_status == 0),
  age_mean = round(mean(clinical_data$age, na.rm=TRUE), 1),
  age_range = range(clinical_data$age, na.rm=TRUE)
)
```

**Clinical summary:**

-   Total samples: `r summary_stats$n_total`
-   AD cases: `r summary_stats$n_ad`
-   Controls: `r summary_stats$n_control`
-   Mean age: `r summary_stats$age_mean` years (range: `r summary_stats$age_range[1]`-`r summary_stats$age_range[2]`)

```{r clinical-tables}
knitr::kable(table(clinical_data$sex), col.names = c("Sex", "N"), 
             caption = "Sex Distribution")

knitr::kable(table(clinical_data$cell_type), col.names = c("Cell Type", "N"),
             caption = "Cell Type Distribution")

knitr::kable(table(clinical_data$braak_stage), col.names = c("Braak Stage", "N"),
             caption = "Braak Stage Distribution")
```

------------------------------------------------------------------------

## **Why Convert Beta Values to M-Values?**

### **The Problem with Beta Values** [5]

Beta values (β) range from 0 to 1, representing the proportion of methylated sites. However, they have statistical limitations:

1.  **Heteroscedasticity**: Variance is not constant across the range
    -   Variance is lowest at β = 0 and β = 1
    -   Variance is highest at β = 0.5
    -   Violates linear regression assumptions
2.  **Boundary constraints**: Values are bounded [0,1]
    -   Can't use standard linear models effectively
    -   Difficult to model fold-changes
3.  **Non-normal distribution**: Often bimodal (many sites near 0 or 1)

### **The M-Value Solution** [6]

M-values are the log₂ ratio of methylated to unmethylated intensities:

$$M = \log_2\left(\frac{\beta}{1-\beta}\right)$$

**Advantages:**

-   **Unbounded**: Range from -∞ to +∞
-   **Homoscedastic**: More constant variance across range
-   **Better statistical properties**: More appropriate for linear modeling
-   **Interpretable**: M-value differences represent log₂ fold-changes

**Interpretation: [5]**

-   M = 0: Equal methylated/unmethylated (β = 0.5)
-   M \> 0: More methylated than unmethylated
-   M \< 0: More unmethylated than methylated
-   Difference of 1 M-value ≈ 2-fold change in methylation ratio

```{r beta-to-m}
# Conversion function
beta_to_m <- function(beta) {
  # Add small offset to avoid log(0)
  beta[beta <= 0] <- 0.001
  beta[beta >= 1] <- 0.999
  m_value <- log2(beta / (1 - beta))
  return(m_value)
}

# Convert all methylation data
m_values <- beta_to_m(methylation_data)
```

**M-value conversion complete:**

-   Range: `r paste(round(range(m_values, na.rm=TRUE), 2), collapse=" to ")`
-   Median: `r round(median(m_values, na.rm=TRUE), 2)`

```{r plot-beta-vs-m, fig.cap="Comparison of Beta and M-value distributions"}
# Sample 10,000 random values for visualization
set.seed(123)
sample_idx <- sample(length(methylation_data), 10000)
sample_beta <- as.vector(methylation_data)[sample_idx]
sample_m <- as.vector(m_values)[sample_idx]

par(mfrow=c(1,2))
hist(sample_beta, breaks=50, main="Beta Values", xlab="Beta", col="lightblue")
hist(sample_m, breaks=50, main="M-Values", xlab="M-value", col="orange")
par(mfrow=c(1,1))
```

------------------------------------------------------------------------

## **Why Perform Feature Selection?**

### **Dimensionality Problem**

We have **481, 868 CpG sites** but only **190 samples**. This creates several problems:

1.  **Overfitting**: Model memorizes training data instead of learning patterns
2.  **Computational burden**: Testing all sites in logistic regression is impractical
3.  **Multiple testing**: Need to account for hundreds of thousands of tests
4.  **Loss of power**: Too many irrelevant features obscure true signals

### **Solution: Two-Stage Analysis**

**Stage 1: Feature Selection** (identify relevant CpGs)

-   Use linear regression to find CpG sites associated with AD progression
-   Apply stringent multiple testing correction

**Stage 2: Predictive Modeling** (build classifier)

-   Use selected CpGs in logistic regression
-   Predict AD diagnosis (binary outcome)

------------------------------------------------------------------------

## **Why Linear Modeling for Feature Selection?**

For each CpG site, we fit a linear model:

$$M_{ij} = \beta_0 + \beta_1 \cdot Braak_i + \beta_2 \cdot Age_i + \beta_3 \cdot Sex_i + \beta_4 \cdot CellType_i + \beta_5 \cdot Region_i + \epsilon_{ij}$$

Where:

-   $M_{ij}$ = M-value for CpG site *j* in sample *i*
-   $Braak_i$ = Braak stage (0-6) for sample *i*
-   Covariates = Age, Sex, Cell type, Brain region
-   $\beta_1$ = effect of AD progression on methylation
-   $\epsilon_{ij}$ = residual error

### **Why This Approach?**

**1. Tests association with disease progression**

-   Braak stage represents AD severity (0 = none, 6 = severe)
-   More sensitive than binary AD/Control comparison
-   Captures dose-response relationship

**2. Established in epigenome-wide association studies (EWAS) [7]**

-   Standard practice in the field
-   Similar to GWAS (genome-wide association studies)
-   Well-validated statistical framework

**3. Identifies mechanistic changes**

-   CpGs that change with disease progression
-   Not just differences between groups
-   Better biological interpretation

------------------------------------------------------------------------

## **Why Include Covariates in Feature Selection?**

### **The Confounding Problem**

Without covariates, we observed too many CpG sites, indicatimng that although these could be differing AD and controls, these differences could also just be due to:

-   **Age differences**: AD patients tend to be older
-   **Sex differences**: AD affects women more than men
-   **Cell type composition**: Different cell types have different methylation
-   **Brain region**: Technical differences between regions

**Ex:** Without covariants, CpG site X is different in AD patients, but maybe it's just because AD patients are older?!

### **Covariate-Controlled Analysis**

By including covariates, we redifine our question: **"Do methylation levels change with AD progression *independen* of age, sex, cell type, and region?"**

This way we are able to analyse:

1.  **True AD-specific effects**: Not confounded by demographics
2.  **Mechanistic insights**: Changes specifically due to disease
3.  **Better generalization**: Model works across different populations
4.  **Reduced false positives**: Don't mistake age effects for AD effects

**Ex:** With covariates, CpG site X changes with AD progression, even after accounting for age differences.

------------------------------------------------------------------------

## **Multiple Testing Correction: FDR vs Bonferroni**

### **The Multiple Testing Problem**

We're testing **481, 868 CpG sites**, each at α = 0.05.

**Expected false positives** under null hypothesis:

$$481,868 \times 0.05 = 24, 093 \text{ false positives!}$$

### **Two Correction Methods**

**1. False Discovery Rate (FDR) [8]**

-   Controls the **proportion** of false discoveries among rejected hypotheses
-   FDR \< 0.05 means \< 5% of significant findings are false positives
-   **Less conservative**, higher power
-   Standard in genomics

**2. Bonferroni Correction [9]**

-   Controls **family-wise error rate** (FWER)
-   Probability of making ANY false positive ≤ α
-   Adjusted p-value threshold: $\alpha / n = 0.05 / 485,577 = 1.03 \times 10^{-7}$
-   **More conservative**, lower power
-   Stricter control

### **Comparison on Our Data**

```{r feature-selection-setup}
# Prepare data
braak_numeric <- as.numeric(clinical_data$braak_stage)
valid_idx <- !is.na(braak_numeric)
clinical_data_valid <- clinical_data[valid_idx, ]
m_values_valid <- m_values[, valid_idx]

cat("Samples after removing missing data:", ncol(m_values_valid), "\n")
```

```{r fit-linear-models, cache=TRUE, results='hide', message=FALSE}
# Function to fit covariate-controlled linear model
fit_cpg_model_covariates <- function(m_vals, clinical) {
  braak <- as.numeric(clinical$braak_stage)
  
  df <- data.frame(
    m_value = m_vals,
    braak_stage = braak,
    age = clinical$age,
    sex = clinical$sex,
    cell_type = clinical$cell_type,
    region = clinical$region
  )
  
  df <- df[complete.cases(df), ]
  
  if (nrow(df) < 20 || length(unique(df$braak_stage)) < 3) {
    return(list(beta = NA, se = NA, t_stat = NA, p_value = NA, 
                adj_r_squared = NA, n_samples = nrow(df)))
  }
  
  tryCatch({
    model <- lm(m_value ~ braak_stage + age + sex + cell_type + region, data = df)
    coef_summary <- summary(model)$coefficients
    
    if (!"braak_stage" %in% rownames(coef_summary)) {
      return(list(beta = NA, se = NA, t_stat = NA, p_value = NA, 
                  adj_r_squared = NA, n_samples = nrow(df)))
    }
    
    braak_row <- which(rownames(coef_summary) == "braak_stage")
    
    return(list(
      beta = coef_summary[braak_row, "Estimate"],
      se = coef_summary[braak_row, "Std. Error"],
      t_stat = coef_summary[braak_row, "t value"],
      p_value = coef_summary[braak_row, "Pr(>|t|)"],
      adj_r_squared = summary(model)$adj.r.squared,
      n_samples = nrow(df)
    ))
  }, error = function(e) {
    return(list(beta = NA, se = NA, t_stat = NA, p_value = NA, 
                adj_r_squared = NA, n_samples = nrow(df)))
  })
}

# Cache file for results
cache_file <- file.path(data_dir, "cpg_results.rds")

if (file.exists(cache_file)) {
  message("✓ Loading cached linear model results...")
  cpg_results <- readRDS(cache_file)
} else {
  message("Fitting linear models for ", nrow(m_values_valid), " CpG sites...")
  message("This will take approximately 10-15 minutes...")
  message("Progress: ", appendLF = FALSE)
  
  results_list <- list()
  progress_step <- round(nrow(m_values_valid) / 50)
  
  for (i in 1:nrow(m_values_valid)) {
    if (i %% progress_step == 0) message(".", appendLF = FALSE)
    results_list[[i]] <- fit_cpg_model_covariates(m_values_valid[i, ], clinical_data_valid)
  }
  message(" Done!")
  
  # Compile results
  cpg_results <- data.frame(
    CpG = rownames(m_values_valid),
    Beta_Coefficient = sapply(results_list, function(x) x$beta),
    Std_Error = sapply(results_list, function(x) x$se),
    t_statistic = sapply(results_list, function(x) x$t_stat),
    p_value = sapply(results_list, function(x) x$p_value),
    adj_r_squared = sapply(results_list, function(x) x$adj_r_squared),
    n_samples = sapply(results_list, function(x) x$n_samples)
  )
  
  # Remove failed models
  cpg_results <- cpg_results[complete.cases(cpg_results), ]
  
  # Save for future runs
  saveRDS(cpg_results, cache_file)
  message("✓ Results cached for future runs")
}

message("Successfully analyzed: ", nrow(cpg_results), " CpG sites")
```

**Linear models fitted for all CpG sites**

> **Note:** This step takes 10-15 minutes on first run but results are cached in `data/cpg_results.rds` for subsequent runs.

-   Successfully analyzed: `r nrow(cpg_results)` CpG sites
-   Failed (insufficient data): `r nrow(m_values_valid) - nrow(cpg_results)`
-   Beta_Coefficient = regression slope (NOT methylation Beta values)
-   These coefficients show M-value change per Braak stage increase, controlling for age, sex, cell type, and region

### **Applying Both Corrections**

```{r multiple-testing}
# Apply both corrections
cpg_results$p_bonferroni <- p.adjust(cpg_results$p_value, method = "bonferroni")
cpg_results$p_fdr <- p.adjust(cpg_results$p_value, method = "fdr")

# Count significant CpGs
n_sig_raw <- sum(cpg_results$p_value < 0.05)
n_sig_fdr <- sum(cpg_results$p_fdr < 0.05)
n_sig_bonf <- sum(cpg_results$p_bonferroni < 0.05)
```

**Significance testing results:**

-   Uncorrected p \< 0.05: **`r n_sig_raw`** (`r sprintf("%.2f%%", 100*n_sig_raw/nrow(cpg_results))` of tested)
-   FDR \< 0.05: **`r n_sig_fdr`** (Benjamini-Hochberg)
-   Bonferroni \< 0.05: **`r n_sig_bonf`** (Family-wise error control)
-   Expected false positives (5%): `r round(nrow(cpg_results) * 0.05)`

```{r comparison-tables}
# Sort by each method
cpg_results_sorted_fdr <- cpg_results[order(cpg_results$p_fdr), ]
cpg_results_sorted_bonf <- cpg_results[order(cpg_results$p_bonferroni), ]

knitr::kable(cpg_results_sorted_fdr[1:10, c("CpG", "Beta_Coefficient", "p_value", "p_fdr")],
             digits = c(0, 4, 12, 6),
             caption = "Top 10 CpGs by FDR",
             row.names = FALSE)

knitr::kable(cpg_results_sorted_bonf[1:10, c("CpG", "Beta_Coefficient", "p_value", "p_bonferroni")],
             digits = c(0, 4, 12, 6),
             caption = "Top 10 CpGs by Bonferroni",
             row.names = FALSE)
```

**Therefore we are choosing Bonferroni:**

1.  **Maximum stringency**: We want only the most robust associations
2.  **Clinical application**: False positives are costly in medical contexts
3.  **Small sample size**: Conservative approach compensates for limited power
4.  **API deployment**: Model must be reliable for external use

> By doing this we are doing a trade-off between **Cost** (Lower power, might miss true associations) and **Benefit** (high confidence in selected markers)

```{r select-cpgs}
# Use Bonferroni for final selection
cpg_results <- cpg_results[order(cpg_results$p_bonferroni), ]

# Calculate appropriate number based on sample size
n_samples <- ncol(m_values_valid)
n_events <- min(sum(clinical_data_valid$AD_status), 
                sum(clinical_data_valid$AD_status == 0))
max_predictors <- floor(n_events / 15)  # 15 samples per predictor rule
max_cpgs <- max(5, max_predictors - 6)  # Account for covariates

# Select CpGs
if (n_sig_bonf == 0) {
  n_to_select <- max_cpgs
} else if (n_sig_bonf <= max_cpgs) {
  n_to_select <- n_sig_bonf
} else {
  n_to_select <- max_cpgs
}

top_cpgs <- cpg_results$CpG[1:n_to_select]
```

**Sample size considerations:**

-   Total samples: `r n_samples`
-   Limiting class size: `r n_events`
-   Maximum predictors (15:1 rule): `r max_predictors`
-   Maximum CpGs (after covariates): `r max_cpgs`
-   **CpGs selected: `r length(top_cpgs)`**

```{r show-selected-cpgs}
# Show selected CpGs with direction of effect
selected_cpg_details <- cpg_results[1:n_to_select, ]
selected_cpg_details$Direction <- ifelse(selected_cpg_details$Beta_Coefficient > 0,
                                         "Hypermethylation", "Hypomethylation")

knitr::kable(selected_cpg_details[1:min(10, n_to_select), 
                                  c("CpG", "Beta_Coefficient", "p_value", 
                                    "p_bonferroni", "Direction")],
             digits = c(0, 4, 12, 6, 0),
             caption = "Top Selected CpG Sites",
             row.names = FALSE)
```

### **Feature Selection Visualizations**

```{r volcano-plot, fig.height=6, fig.width=10, fig.cap="Volcano plot showing effect size vs statistical significance for all tested CpG sites"}
# Volcano plot
cpg_results$log10p <- -log10(cpg_results$p_value)
cpg_results$selected <- cpg_results$CpG %in% top_cpgs

bonferroni_threshold <- -log10(0.05 / nrow(cpg_results))

ggplot(cpg_results, aes(x = Beta_Coefficient, y = log10p, color = selected)) +
  geom_point(alpha = 0.4, size = 1) +
  geom_hline(yintercept = bonferroni_threshold, 
             linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = -log10(0.05), 
             linetype = "dotted", color = "turquoise", alpha = 0.5) +
  scale_color_manual(values = c("gray70", "gold"),
                     labels = c("Not selected", "Selected for model"),
                     name = "") +
  labs(title = "Volcano Plot: Feature Selection Results",
       subtitle = paste("Testing", nrow(cpg_results), 
                       "CpG sites | Red line = Bonferroni threshold | Blue line = p=0.05"),
       x = "Effect Size (Beta coefficient for Braak stage)",
       y = "-log10(p-value)") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        legend.position = "top")
```

```{r bar-plot-cpgs, fig.height=8, fig.width=10, fig.cap="Bar plot showing effect direction and magnitude for selected CpG sites"}
# Bar plot of selected CpGs
plot_cpgs <- cpg_results[cpg_results$CpG %in% top_cpgs, ]
plot_cpgs$CpG <- factor(plot_cpgs$CpG, 
                        levels = plot_cpgs$CpG[order(plot_cpgs$Beta_Coefficient)])

ggplot(plot_cpgs, aes(x = CpG, y = Beta_Coefficient, fill = Beta_Coefficient > 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = c("TRUE" = "lightblue", "FALSE" = "orange"),
                    labels = c("Decreases with progression", 
                               "Increases with progression")) +
  labs(title = "Selected CpG Sites: Effect on Methylation",
       subtitle = "M-value change per Braak stage increase",
       x = "CpG Site",
       y = "Beta Coefficient (M-value change per Braak stage)",
       fill = "Direction") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8)) +
  geom_hline(yintercept = 0, linetype = "dashed")
```

```{r linear-models-grid, fig.height=8, fig.width=12, fig.cap="Linear regression fits for top CpG sites showing relationship with Braak stage"}
# Plot linear models for top 6 CpGs
top_cpgs <- top_cpgs[1:min(6, length(top_cpgs))]


# Prepare plotting data
braak_for_plot <- as.numeric(clinical_data_valid$braak_stage)

plot_data_list <- list()
for (cpg in top_cpgs) {
  plot_data_list[[cpg]] <- data.frame(
    CpG = cpg,
    M_value = m_values_valid[cpg, ],
    Braak_stage = braak_for_plot
  )
}

plot_data <- do.call(rbind, plot_data_list)
plot_data <- plot_data[complete.cases(plot_data), ]

ggplot(plot_data, aes(x = Braak_stage, y = M_value)) +
  geom_point(alpha = 0.6, size = 2, color = "gray40") +
  geom_smooth(method = "lm", se = TRUE, color = "red", fill = "#ff7f0e", 
              alpha = 0.2, linewidth = 1.2) +
  facet_wrap(~CpG, scales = "free_y", ncol = 3) +
  scale_x_continuous(breaks = 0:6) +
  labs(title = "Linear Models: Top CpG Sites vs AD Progression",
       subtitle = "Model: M-value ~ Braak_stage + age + sex + cell_type + region",
       x = "Braak Stage (0 = Control, 6 = Severe AD)",
       y = "M-value (log2 methylation ratio)",
       caption = "Red line = fitted linear model; Orange band = 95% confidence interval") +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "lightgray", color = "black"),
    panel.grid.minor = element_blank()
  )
```

```{r best-cpg-detail, fig.height=6, fig.width=10, fig.cap="Detailed view of the most significant CpG site with regression equation"}
# Detailed plot for best CpG
best_cpg <- top_cpgs[1]
best_cpg_data <- plot_data[plot_data$CpG == best_cpg, ]

# Fit simple model for display
best_model_simple <- lm(M_value ~ Braak_stage, data = best_cpg_data)
intercept <- coef(best_model_simple)[1]
slope <- coef(best_model_simple)[2]
r_squared <- summary(best_model_simple)$r.squared
p_val <- summary(best_model_simple)$coefficients[2, 4]

equation_text <- paste0(
  "M = ", round(intercept, 3), " + ", round(slope, 3), " × Braak\n",
  "R² = ", round(r_squared, 3), ", p = ", signif(p_val, 3)
)

ggplot(best_cpg_data, aes(x = Braak_stage, y = M_value)) +
  geom_jitter(alpha = 0.6, size = 3, width = 0.1, height = 0, color = "gray40") +
  geom_smooth(method = "lm", se = TRUE, color = "red", fill = "#ff7f0e", 
              alpha = 0.2, linewidth = 1.5) +
  scale_x_continuous(breaks = 0:6) +
  labs(title = paste("Most Significant CpG Site:", best_cpg),
       subtitle = "Simple linear regression for visualization (full model includes covariates)",
       x = "Braak Stage (AD Severity: 0 = None, 6 = Severe)",
       y = "M-value (Methylation Level)",
       caption = "Each point = one sample; Line = linear regression fit") +
  annotate("text", x = 1, y = max(best_cpg_data$M_value) * 0.95, 
           label = equation_text, hjust = 0, size = 4.5, 
           fontface = "italic", color = "black") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11),
    axis.title = element_text(size = 11, face = "bold")
  )
```

------------------------------------------------------------------------

## **Split Data into Training and Test Sets**

### **The Generalization Problem**

Since our goal is to build a model that works on **new, unseen data**, we need proper validation to make sure our model **Learned patterns** and is not just **Memoring training data**:

-   **Training set (70%)**: Used to fit the model, learn coefficients and optimize parameters.

-   **Test set (30%)**: Used to evaluate performance in completely independent data that the model has never seen these samples, creating and honest assessment of generalization.

> 70/30 Split is a **Common standard** in machine learning that **balances**: \* Training set: Large enough to learn patterns \* Test set: Large enough for reliable evaluation

```{r train-test-split, results='hide'}
# Prepare final dataset
selected_m_values <- t(m_values_valid[top_cpgs, ])
colnames(selected_m_values) <- paste0("M_", 1:length(top_cpgs))

analysis_data <- cbind(clinical_data_valid, selected_m_values)
analysis_data <- analysis_data[complete.cases(analysis_data), ]

# Stratified split (maintains AD/Control ratio)
set.seed(123)
train_idx <- createDataPartition(analysis_data$AD_status, p = 0.7, list = FALSE)

train_data <- analysis_data[train_idx, ]
test_data <- analysis_data[-train_idx, ]
```

**Training set:**

-   Samples: `r nrow(train_data)`
-   AD cases: `r sum(train_data$AD_status)` (`r sprintf("%.1f%%", 100*mean(train_data$AD_status))`)
-   Controls: `r sum(train_data$AD_status == 0)` (`r sprintf("%.1f%%", 100*mean(train_data$AD_status == 0))`)

**Test set:**

-   Samples: `r nrow(test_data)`
-   AD cases: `r sum(test_data$AD_status)` (`r sprintf("%.1f%%", 100*mean(test_data$AD_status))`)
-   Controls: `r sum(test_data$AD_status == 0)` (`r sprintf("%.1f%%", 100*mean(test_data$AD_status == 0))`)

✓ Balanced split achieved

------------------------------------------------------------------------

# **Logistic Regression Model**

## **Model Development**

```{r fit-logistic-model, results='hide', message=FALSE}
# Create model formula
m_vars <- paste0("M_", 1:length(top_cpgs))
formula_str <- paste("AD_status ~", 
                     paste(c(m_vars, "age", "sex", "cell_type", "region"), 
                           collapse = " + "))
model_formula <- as.formula(formula_str)

# Fit model on TRAINING DATA ONLY
model <- glm(model_formula, data = train_data, family = "binomial")
```

**Logistic regression model fitted on training data**

Formula: `AD_status ~ M_1 + M_2 + ... + M_`r length(top_cpgs)`+ age + sex + cell_type + region`

```{r model-summary, comment=''}
summary(model)
```

------------------------------------------------------------------------

## **Coefficient Interpretation**

### **Understanding Logistic Regression Output**

**The model estimates:**

$$\log\left(\frac{P(AD)}{1-P(AD)}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p$$

**Interpreting coefficients:**

-   **Coefficient (β)**: Log-odds change per unit increase
    -   Positive β: Increases AD risk
    -   Negative β: Decreases AD risk
-   **Odds Ratio (OR = e\^β)**: Multiplicative effect on odds
    -   OR \> 1: Risk factor
    -   OR \< 1: Protective factor
    -   OR = 1: No effect

**Example:**

-   Coefficient = 0.693 → OR = e\^0.693 = 2.0
-   Interpretation: Each unit increase **doubles** the odds of AD

```{r coefficient-interpretation}
# Extract coefficients
coef_summary <- summary(model)$coefficients
odds_ratios <- exp(coef(model))

# Calculate confidence intervals
conf_intervals <- tryCatch({
  ci <- confint(model, level = 0.95)
  exp(ci)
}, error = function(e) {
  cat("Using Wald approximation for confidence intervals\n\n")
  coef_est <- coef(model)
  se <- coef_summary[, "Std. Error"]
  lower <- exp(coef_est - 1.96 * se)
  upper <- exp(coef_est + 1.96 * se)
  ci <- cbind(lower, upper)
  colnames(ci) <- c("2.5 %", "97.5 %")
  ci
})

# Align by coefficient names
coef_names <- rownames(coef_summary)
odds_ratios_aligned <- odds_ratios[coef_names]
conf_intervals_aligned <- conf_intervals[coef_names, , drop = FALSE]

# Create interpretation table
interpretation <- data.frame(
  Variable = coef_names,
  Coefficient = round(coef_summary[, "Estimate"], 4),
  Odds_Ratio = round(odds_ratios_aligned, 4),
  Lower_95CI = round(conf_intervals_aligned[, 1], 4),
  Upper_95CI = round(conf_intervals_aligned[, 2], 4),
  P_value = round(coef_summary[, "Pr(>|z|)"], 4),
  Significant = ifelse(coef_summary[, "Pr(>|z|)"] < 0.05, "***",
                      ifelse(coef_summary[, "Pr(>|z|)"] < 0.1, "*", ""))
)

# Remove intercept
interpretation_predictors <- interpretation[interpretation$Variable != "(Intercept)", ]
```

### **Full Coefficient Table**

```{r coefficient-table}
knitr::kable(interpretation_predictors,
             caption = "Odds Ratios and 95% Confidence Intervals for All Predictors",
             digits = 4,
             row.names = FALSE)
```

**Interpretation guide:**

-   **Coefficient**: Log-odds change (β in model equation)
    -   Positive → increases AD risk
    -   Negative → decreases AD risk
-   **Odds Ratio**: Multiplicative effect on AD odds
    -   OR \> 1: Risk factor (e.g., OR=2.0 doubles odds)
    -   OR \< 1: Protective (e.g., OR=0.5 halves odds)
    -   OR = 1: No effect
-   **95% CI**: Confidence interval for OR
    -   If excludes 1.0 → statistically significant
-   **P-value**: Statistical significance
    -   p \< 0.05: Significant (**\***)
    -   p \< 0.10: Marginally significant (\*)

### **Detailed Interpretation of Key Predictors**

```{r detailed-interpretation}
# Methylation markers
m_predictors <- interpretation_predictors[grepl("^M_", interpretation_predictors$Variable), ]

if (nrow(m_predictors) > 0) {
  m_predictors <- m_predictors[order(abs(m_predictors$Coefficient), decreasing = TRUE), ]
  
  cat("\n═══════════════════════════════════════════════════════════════\n")
  cat("METHYLATION MARKERS (Top CpG Sites)\n")
  cat("═══════════════════════════════════════════════════════════════\n\n")
  
  for (i in 1:min(5, nrow(m_predictors))) {
    var <- m_predictors[i, ]
    cpg_name <- top_cpgs[as.numeric(gsub("M_", "", var$Variable))]
    
    cat(sprintf("\n%d. %s (CpG: %s)\n", i, var$Variable, cpg_name))
    cat(strrep("─", 65), "\n")
    
    if (var$Odds_Ratio > 1) {
      percent_increase <- (var$Odds_Ratio - 1) * 100
      cat(sprintf("Effect: INCREASES AD risk\n"))
      cat(sprintf("Odds Ratio: %.3f (%.1f%% increase)\n", 
                  var$Odds_Ratio, percent_increase))
      cat(sprintf("Interpretation: Each 1-unit increase in M-value\n"))
      cat(sprintf("  multiplies AD odds by %.2f\n", var$Odds_Ratio))
    } else {
      percent_decrease <- (1 - var$Odds_Ratio) * 100
      cat(sprintf("Effect: DECREASES AD risk (protective)\n"))
      cat(sprintf("Odds Ratio: %.3f (%.1f%% decrease)\n", 
                  var$Odds_Ratio, percent_decrease))
      cat(sprintf("Interpretation: Each 1-unit increase in M-value\n"))
      cat(sprintf("  multiplies AD odds by %.2f (protective)\n", var$Odds_Ratio))
    }
    
    cat(sprintf("95%% CI: [%.3f, %.3f]\n", var$Lower_95CI, var$Upper_95CI))
    cat(sprintf("P-value: %.4f %s\n", var$P_value, var$Significant))
    
    if (var$P_value < 0.05) {
      cat("✓ Statistically significant predictor\n")
    } else {
      cat("○ Not statistically significant in full model\n")
      cat("  (Selected by feature selection, but effect attenuated by covariates)\n")
    }
  }
}

# Covariate effects
covariate_predictors <- interpretation_predictors[!grepl("^M_", interpretation_predictors$Variable), ]

if (nrow(covariate_predictors) > 0) {
  cat("\n\n═══════════════════════════════════════════════════════════════\n")
  cat("COVARIATE EFFECTS\n")
  cat("═══════════════════════════════════════════════════════════════\n")
  
  for (i in 1:nrow(covariate_predictors)) {
    var <- covariate_predictors[i, ]
    cat(sprintf("\n%s\n", var$Variable))
    cat(strrep("─", 50), "\n")
    
    if (grepl("^age$", var$Variable, ignore.case = TRUE)) {
      if (var$Odds_Ratio > 1) {
        cat(sprintf("Each additional year multiplies AD odds by %.4f\n", var$Odds_Ratio))
        cat(sprintf("Over 10 years: %.2f× odds increase\n", var$Odds_Ratio^10))
      }
    } else if (grepl("sex|cell_type|region", var$Variable, ignore.case = TRUE)) {
      cat(sprintf("Odds ratio vs reference: %.3f\n", var$Odds_Ratio))
      if (var$Odds_Ratio > 1) {
        cat("Higher odds than reference category\n")
      } else {
        cat("Lower odds than reference category\n")
      }
    }
    
    cat(sprintf("P-value: %.4f %s\n", var$P_value, var$Significant))
  }
}
```

------------------------------------------------------------------------

# **Model Evaluation**

## **Performance Metrics Formulas**

**Accuracy**: Overall correctness = (TP + TN) / Total

**Sensitivity (Recall)**: Ability to detect AD cases = TP / (TP + FN)

-   Important for screening: Don't want to miss AD patients

**Specificity**: Ability to identify controls = TN / (TN + FP)

-   Important to avoid false alarms

**Positive Predictive Value (PPV)**: If test says "AD", probability it's correct = TP / (TP + FP)

**Negative Predictive Value (NPV)**: If test says "Control", probability it's correct = TN / (TN + FN)

## **Performance Metrics**

```{r model-evaluation, results='hide'}
# Predictions on training set
train_probs <- predict(model, type = "response")
train_pred <- ifelse(train_probs > 0.5, 1, 0)

# Predictions on test set
test_probs <- predict(model, newdata = test_data, type = "response")
test_pred <- ifelse(test_probs > 0.5, 1, 0)

# Training performance
train_confusion <- table(Predicted = train_pred, Actual = train_data$AD_status)
train_accuracy <- sum(diag(train_confusion)) / sum(train_confusion)
train_sensitivity <- train_confusion[2, 2] / sum(train_confusion[, 2])
train_specificity <- train_confusion[1, 1] / sum(train_confusion[, 1])
train_ppv <- train_confusion[2, 2] / sum(train_confusion[2, ])
train_npv <- train_confusion[1, 1] / sum(train_confusion[1, ])

# Test performance
test_confusion <- table(Predicted = test_pred, Actual = test_data$AD_status)
test_accuracy <- sum(diag(test_confusion)) / sum(test_confusion)
test_sensitivity <- test_confusion[2, 2] / sum(test_confusion[, 2])
test_specificity <- test_confusion[1, 1] / sum(test_confusion[, 1])
test_ppv <- test_confusion[2, 2] / sum(test_confusion[2, ])
test_npv <- test_confusion[1, 1] / sum(test_confusion[1, ])
```

### **Training Set Performance**

```{r train-confusion}
knitr::kable(train_confusion, caption = "Training Set Confusion Matrix")
```

-   **Accuracy**: `r sprintf("%.2f%%", train_accuracy * 100)`
-   **Sensitivity**: `r sprintf("%.2f%%", train_sensitivity * 100)` (true positive rate)
-   **Specificity**: `r sprintf("%.2f%%", train_specificity * 100)` (true negative rate)
-   **PPV**: `r sprintf("%.2f%%", train_ppv * 100)` (precision)
-   **NPV**: `r sprintf("%.2f%%", train_npv * 100)`

### **Test Set Performance**

```{r test-confusion}
knitr::kable(test_confusion, caption = "Test Set Confusion Matrix")
```

-   **Accuracy**: `r sprintf("%.2f%%", test_accuracy * 100)`
-   **Sensitivity**: `r sprintf("%.2f%%", test_sensitivity * 100)` (true positive rate)
-   **Specificity**: `r sprintf("%.2f%%", test_specificity * 100)` (true negative rate)
-   **PPV**: `r sprintf("%.2f%%", test_ppv * 100)` (precision)
-   **NPV**: `r sprintf("%.2f%%", test_npv * 100)`

### **Generalization Assessment**

-   Training accuracy: `r sprintf("%.2f%%", train_accuracy * 100)`
-   Test accuracy: `r sprintf("%.2f%%", test_accuracy * 100)`
-   Difference: `r sprintf("%.2f%%", (train_accuracy - test_accuracy) * 100)`

`r if(train_accuracy - test_accuracy > 0.10) "⚠ Warning: >10% drop suggests overfitting" else if(train_accuracy - test_accuracy < -0.05) "✓ Test performance exceeds training (unusual but good!)" else "✓ Good generalization - model performs consistently"`

------------------------------------------------------------------------

## **Visualizations**

```{r plot-predictions, fig.height=6, fig.width=10}
# Predicted probability distributions
prob_df <- data.frame(
  Probability = c(train_probs, test_probs),
  Actual = factor(c(train_data$AD_status, test_data$AD_status), 
                  labels = c("Control", "AD")),
  Dataset = c(rep("Training", length(train_probs)), 
              rep("Test", length(test_probs)))
)

ggplot(prob_df, aes(x = Probability, fill = Actual)) +
  geom_histogram(alpha = 0.6, position = "identity", bins = 30) +
  facet_wrap(~Dataset) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  labs(title = "Distribution of Predicted Probabilities",
       subtitle = "Red line = decision threshold (0.5)",
       x = "Predicted Probability of AD",
       y = "Count",
       fill = "Actual Diagnosis") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"))
```

```{r plot-odds-ratios, fig.height=8, fig.width=10}
# Plot odds ratios for methylation markers
m_predictors_plot <- interpretation_predictors[grepl("^M_", interpretation_predictors$Variable), ]

if (nrow(m_predictors_plot) > 0) {
  # Add CpG names
  m_predictors_plot$CpG_name <- sapply(m_predictors_plot$Variable, function(x) {
    idx <- as.numeric(gsub("M_", "", x))
    top_cpgs[idx]
  })
  
  m_predictors_plot$Label <- paste0(m_predictors_plot$Variable, "\n", 
                                    m_predictors_plot$CpG_name)
  m_predictors_plot$Label <- factor(m_predictors_plot$Label,
                                    levels = m_predictors_plot$Label[order(m_predictors_plot$Odds_Ratio)])
  
  ggplot(m_predictors_plot, aes(x = Odds_Ratio, y = Label)) +
    geom_point(size = 4, aes(color = P_value < 0.05)) +
    geom_errorbarh(aes(xmin = Lower_95CI, xmax = Upper_95CI), height = 0.3) +
    geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
    scale_x_log10(breaks = c(0.01, 0.1, 1, 10, 100)) +
    scale_color_manual(values = c("FALSE" = "gray50", "TRUE" = "orange"),
                       labels = c("p ≥ 0.05", "p < 0.05"),
                       name = "Significance") +
    labs(title = "Methylation Markers: Odds Ratios with 95% Confidence Intervals",
         subtitle = "Red line = no effect (OR = 1.0); log scale",
         x = "Odds Ratio (log scale)",
         y = "Methylation Marker") +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"),
          axis.text.y = element_text(size = 8))
}
```

```{r plot-volcano, fig.height=6, fig.width=10}
# Volcano plot of feature selection
cpg_results$log10p <- -log10(cpg_results$p_value)
cpg_results$selected <- cpg_results$CpG %in% top_cpgs

bonferroni_threshold <- -log10(0.05 / nrow(cpg_results))

ggplot(cpg_results, aes(x = Beta_Coefficient, y = log10p, color = selected)) +
  geom_point(alpha = 0.4, size = 1) +
  geom_hline(yintercept = bonferroni_threshold, 
             linetype = "dashed", color = "red", size = 1) +
  scale_color_manual(values = c("gray70", "darkgreen"),
                     labels = c("Not selected", "Selected for model"),
                     name = "") +
  labs(title = "Volcano Plot: Feature Selection Results",
       subtitle = paste("Testing", nrow(cpg_results), 
                       "CpG sites | Red line = Bonferroni threshold"),
       x = "Effect Size (Beta coefficient for Braak stage)",
       y = "-log10(p-value)") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        legend.position = "top")
```

------------------------------------------------------------------------

# **Saving Model for API Deployment**

```{r save-model}
# Save the complete model object
saveRDS(model, file = "alzheimers_model.rds")

cat("✓ Model saved as: alzheimers_model.rds\n\n")

cat("Selected CpG sites for API:\n")
print(data.frame(
  Index = 1:length(top_cpgs),
  CpG_ID = top_cpgs
))

# Also save the list of required CpGs
saveRDS(top_cpgs, file = "selected_cpgs.rds")
cat("\n✓ Selected CpG list saved as: selected_cpgs.rds\n")

# Extract interpretation table (odds ratios for all predictors)
coef_summary <- summary(model)$coefficients
odds_ratios <- exp(coef(model))
conf_intervals <- exp(confint(model, level = 0.95))

interpretation <- data.frame(
  Variable = rownames(coef_summary),
  Coefficient = coef_summary[, "Estimate"],
  Odds_Ratio = odds_ratios[rownames(coef_summary)],
  Lower_95CI = conf_intervals[rownames(coef_summary), 1],
  Upper_95CI = conf_intervals[rownames(coef_summary), 2],
  P_value = coef_summary[, "Pr(>|z|)"]
)
interpretation <- interpretation[interpretation$Variable != "(Intercept)", ]

# Save complete API data
saveRDS(list(
  model = model,
  top_cpgs = top_cpgs,
  interpretation = interpretation,
  beta_to_m = beta_to_m,
  metrics = list(
    train_accuracy = train_accuracy,
    test_accuracy = test_accuracy,
    train_sensitivity = train_sensitivity,
    test_sensitivity = test_sensitivity,
    train_specificity = train_specificity,
    test_specificity = test_specificity,
    train_ppv = train_ppv,
    test_ppv = test_ppv,
    train_npv = train_npv,
    test_npv = test_npv,
    train_confusion = train_confusion,
    test_confusion = test_confusion
  )
), "alzheimers_model_api.rds")
```

## **To load the model in API:**

cat(" model \<- readRDS('alzheimers_model.rds')\n\n")

### **Model Object Contains**

-   Fitted coefficients
-   Variable names and structure
-   Model formula
-   All information required for prediction

### **Required Inputs for API Predictions**

To generate predictions, the API must be provided with:

-   **M-values** for the `r length(top_cpgs)` selected CpG sites
-   **Age** (continuous)
-   **Sex** (factor)
-   **Cell type** (factor)
-   **Brain region** (factor)

------------------------------------------------------------------------

# **Discussion**

## **Key Findings**

### **Feature Selection Results**

-   **Tested**: `r nrow(cpg_results)` CpG sites genome-wide
-   **Bonferroni significant**: `r n_sig_bonf` sites
-   **Selected for model**: `r length(top_cpgs)` sites (limited by sample size)

The selected CpG sites show methylation changes associated with AD progression, independent of age, sex, cell type, and brain region.

### **Model Performance**

**Test set results** (most important for evaluating generalization):

-   Accuracy: `r sprintf("%.1f%%", test_accuracy * 100)`
-   Sensitivity: `r sprintf("%.1f%%", test_sensitivity * 100)`
-   Specificity: `r sprintf("%.1f%%", test_specificity * 100)`

The model shows `r if(train_accuracy - test_accuracy > 0.10) "signs of overfitting" else "good generalization"` from training to test data.

### **Risk Factors Identified**

The logistic regression identified several methylation markers that significantly contribute to AD risk, after controlling for demographic factors. See the coefficient interpretation section for details.

------------------------------------------------------------------------

## **Limitations**

### **Post-Mortem Tissue**

-   Brain tissue collected after death
-   Cannot use for early diagnosis
-   Retrospective analysis only

### **Generalization Concerns**

-   Single cohort from one study
-   Specific brain regions only
-   Need validation in independent datasets

------------------------------------------------------------------------

# **Conclusions**

With this project we successfully developed a logistic regression model for Alzheimer's Disease prediction using DNA methylation markers from brain tissue using the GSE66351 dataset.

**Main achievements:**

1.  Identified methylation changes associated with AD progression
2.  Controlled for demographic and technical confounders
3.  Applied stringent multiple testing correction (Bonferroni)
4.  Validated model on independent test set
5.  Saved model for API deployment

**Our main takeaways are that:**

-   DNA methylation patterns differ between AD and control brains
-   These differences persist after controlling for age, sex, cell type, and region
-   A subset of CpG sites can predict AD status with reasonable accuracy
-   While this model uses post-mortem brain tissue (limiting immediate clinical use), it is able to identify potential therapeutic targets, suggest biomarkers for blood-based tests. This improves our understanding of AD epigenetics and provides foundation for future diagnostic tools

------------------------------------------------------------------------

# **References**

1.  König, T. and Stögmann, E. (2021). Genetics of Alzheimer’s disease. Wiener Medizinische Wochenschrift, 171(11-12). <doi:https://doi.org/10.1007/s10354-021-00819-9>.

2.  Moore, L.D., Le, T. and Fan, G. (2012). DNA Methylation and Its Basic Function. Neuropsychopharmacology, 38(1), pp.23–38. <doi:https://doi.org/10.1038/npp.2012.112>.

3.  Gan, Y., Sun, J., Yang, D., Fang, C., Zhou, Z. and Yin, J. (2025). Exploration and validation of biomarkers for Alzheimer’s disease based on GEO database. IBRO Neuroscience Reports. [online] <doi:https://doi.org/10.1016/j.ibneur.2025.03.006>.

4.  Nih.gov. (2024). GEO Accession viewer. [online] Available at: <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE66351> [Accessed 17 Dec. 2025].

5.  Du, P., Zhang, X., Huang, C.-C., Jafari, N., Kibbe, W.A., Hou, L. and Lin, S.M. (2010). Comparison of Beta-value and M-value methods for quantifying methylation levels by microarray analysis. BMC Bioinformatics, 11(1). <doi:https://doi.org/10.1186/1471-2105-11-587>.

6.  life-epigenetics-methylprep.readthedocs-hosted.com. (n.d.). Introduction to DNA Methylation Analysis — methylprep 1.6.5 documentation. [online] Available at: <https://life-epigenetics-methylprep.readthedocs-hosted.com/en/latest/docs/introduction/introduction.html>.

7.  Staley, J.R., Suderman, M., Simpkin, A.J., Gaunt, T.R., Heron, J., Relton, C.L. and Tilling, K. (2018). Longitudinal analysis strategies for modelling epigenetic trajectories. International Journal of Epidemiology, 47(2), pp.516–525. <doi:https://doi.org/10.1093/ije/dyy012>.

8.  www.publichealth.columbia.edu. (n.d.). False Discovery Rate \| Columbia Public Health. [online] Available at: <https://www.publichealth.columbia.edu/research/population-health-methods/false-discovery-rate>.

9.  Armstrong, R.A. (2014). When to use the Bonferroni correction. Ophthalmic and Physiological Optics, [online] 34(5), pp.502–508. <doi:https://doi.org/10.1111/opo.12131>.
